{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Read Data from the Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets__file = \"Data/PositiveTweets.tsv\"\n",
    "negative_tweets__file = \"Data/NegativeTweets.tsv\"\n",
    "bad_words__file = \"Data/bad_words.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.read_csv(negative_tweets__file, sep='\\t', header=None, names=[\"sentiment\", \"content\"], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv(positive_tweets__file, sep='\\t', header=None, names=[\"sentiment\", \"content\"], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([neg_df, pos_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>#Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22756</th>\n",
       "      <td>pos</td>\n",
       "      <td>Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù„ÙŠÙ„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙÙˆÙ† .. Ø±ØªÙˆÙŠØª Ù„Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØ·Ø¨Ù‚...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22757</th>\n",
       "      <td>pos</td>\n",
       "      <td>ğŸ˜‚ Ù„Ø§Ø¨Ø³Ø© Ø§Ø­Ù…Ø± Ù„ÙŠÙ‡ ÙŠØ§ Ø³Øª Ø§Ù†ØªÙŠ Ø§ÙŠÙ‡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22758</th>\n",
       "      <td>pos</td>\n",
       "      <td>ÙƒÙ„Ø§Ø§Ù… Ø¬Ù…ÙŠÙŠÙ„ ØªØ³ØªØ§Ù‡Ù„(Ù…Ù† Ø§Ø­Ø¨Ù‡ Ø§Ù„Ù„Ù‡ Ø¬Ø¹Ù„ Ù…Ø­Ø¨ØªÙ‡ Ù Ù‚Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>pos</td>\n",
       "      <td>- Ø£Ù„Ø·Ù ØµÙˆØ±Ø© Ù…Ù…ÙƒÙ† ØªØ¹Ø¨Ø± Ø¹Ù† Ø±Ù…Ø¶Ø§Ù† ğŸ’™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22760</th>\n",
       "      <td>pos</td>\n",
       "      <td>ğŸŒ¸ Ù‚Ø§Ù„ #Ø§Ù„Ø¥Ù…Ø§Ù…_Ø§Ø¨Ù†_Ø§Ù„Ù‚ÙŠÙ… -Ø±Ø­Ù…Ù‡ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰- : - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                            content\n",
       "0           neg  Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...\n",
       "1           neg  ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...\n",
       "2           neg  #Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...\n",
       "3           neg  Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...\n",
       "4           neg                             Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”\n",
       "...         ...                                                ...\n",
       "22756       pos  Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù„ÙŠÙ„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙÙˆÙ† .. Ø±ØªÙˆÙŠØª Ù„Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØ·Ø¨Ù‚...\n",
       "22757       pos         ğŸ˜‚ Ù„Ø§Ø¨Ø³Ø© Ø§Ø­Ù…Ø± Ù„ÙŠÙ‡ ÙŠØ§ Ø³Øª Ø§Ù†ØªÙŠ Ø§ÙŠÙ‡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ğŸ˜‚\n",
       "22758       pos  ÙƒÙ„Ø§Ø§Ù… Ø¬Ù…ÙŠÙŠÙ„ ØªØ³ØªØ§Ù‡Ù„(Ù…Ù† Ø§Ø­Ø¨Ù‡ Ø§Ù„Ù„Ù‡ Ø¬Ø¹Ù„ Ù…Ø­Ø¨ØªÙ‡ Ù Ù‚Ù„...\n",
       "22759       pos                   - Ø£Ù„Ø·Ù ØµÙˆØ±Ø© Ù…Ù…ÙƒÙ† ØªØ¹Ø¨Ø± Ø¹Ù† Ø±Ù…Ø¶Ø§Ù† ğŸ’™\n",
       "22760       pos  ğŸŒ¸ Ù‚Ø§Ù„ #Ø§Ù„Ø¥Ù…Ø§Ù…_Ø§Ø¨Ù†_Ø§Ù„Ù‚ÙŠÙ… -Ø±Ø­Ù…Ù‡ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰- : - ...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = set()\n",
    "with open(bad_words__file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        bad_words.add(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Adding Features**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_length_before\"] = df[\"content\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_count_before\"] = df[\"content\"].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences_count_before\"] = df[\"content\"].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hashtags_count\"] = df[\"content\"].apply(lambda x: len(re.findall(r'#', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bad_words_count\"] = df[\"content\"].apply(lambda x: len([word for word in word_tokenize(x) if word in bad_words]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emojis_count\"] = df[\"content\"].apply(lambda x: emoji.emoji_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_emojis = ['â£', 'ğŸ’', 'ğŸ¤', 'ğŸ’Œ', 'ğŸ§¡', 'ğŸ’™', 'ğŸ’›', 'ğŸ¤', 'ğŸ’—', 'ğŸ’“', 'ğŸ’‹', 'ğŸ’', 'ğŸ’š', 'ğŸ–¤', 'ğŸ’Ÿ', 'ğŸ”¥',\n",
    "                '\\u200d', 'ğŸ©¹', 'ğŸ’˜', 'ğŸ’œ', 'â¤', 'ğŸ’Ÿ', 'ğŸ’', 'ğŸ’•', 'ğŸ’–', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'â™¥ï¸', 'ğŸ˜š', 'ğŸ˜½',\n",
    "                'ğŸ˜‡', 'ğŸ«¶', 'ğŸ˜Š', 'â˜ºï¸', 'ğŸ¤—', 'ğŸ¤©', 'ğŸ¥°', 'ğŸ˜»', 'ğŸ™ˆ', 'ğŸ™Š', 'ğŸ«¦', 'ğŸ‘„', 'ğŸ«€', 'ğŸ’', 'ğŸ’‘', 'ğŸ’‹']\n",
    "\n",
    "df['love_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in love_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_heart_emoji = ['ğŸ’”']\n",
    "\n",
    "df['broken_heart'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in broken_heart_emoji)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_emojis = ['ğŸ˜„', 'ğŸ˜¹', 'ğŸ˜ƒ', 'ğŸ˜†', 'ğŸ˜…', 'ğŸ˜€', 'ğŸ¤£', 'ğŸ˜‚', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜¸', 'ğŸ˜‡', 'ğŸ¤ª', 'ğŸ˜', 'ğŸ•º', 'ğŸ’ƒ']\n",
    "\n",
    "df['happy_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in happy_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_emojis = ['\\U0001fae4', 'ğŸ¥²', 'ğŸ˜Ÿ', 'ğŸ˜©', 'ğŸ˜¢', 'ğŸ˜“', 'ğŸ˜¥', 'ğŸ™ƒ', 'ğŸ’…', 'ğŸ˜', 'ğŸ˜”', 'ğŸ™', 'â˜¹ï¸', 'ğŸ˜¿', 'ğŸ˜–',\n",
    "                'ï¸ğŸ’”', 'ğŸ˜°', 'ğŸ˜­', 'ğŸ˜£', 'â˜¹', 'ğŸ˜•', 'ğŸ¥µ', 'ğŸ¥´', 'ğŸ¤•', 'ğŸ¤’']\n",
    "\n",
    "df['sad_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in sad_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_emojis = ['ğŸ™‚', 'ğŸ™ƒ']\n",
    "\n",
    "df['smile_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in smile_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_emojis = ['ğŸ¤”', 'ğŸ¤¨', 'ğŸ§', 'ğŸ¤“']\n",
    "\n",
    "df['thinking_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in thinking_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_emojis = ['ğŸ’', 'ğŸŒ¸', 'ğŸµï¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ·', 'ğŸ¥€', 'â˜˜ï¸', 'ğŸ']\n",
    "\n",
    "df['flowers_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in flowers_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_and_sun_emojis = ['ğŸŒ', 'ğŸŒ‘', 'ğŸŒ’', 'ğŸŒ“', 'ğŸŒ”', 'ğŸŒ•', 'ğŸŒ–', 'ğŸŒ—', 'ğŸŒ˜', 'ğŸŒ™', 'ğŸŒš', 'ğŸŒ›', 'ğŸŒœ', 'â˜€ï¸', 'ğŸŒ',\n",
    "                        'â­', 'ğŸŒŸ', 'ğŸŒ ', 'âœ¨']\n",
    "\n",
    "df['moon_and_sun_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in moon_and_sun_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_emojis = ['ğŸ‘‰', 'âœŠ', 'ğŸ‘Œ', '\\U0001faf5', 'ğŸ¤˜', 'ğŸ¤', 'ğŸ¤', '\\U0001faf1', 'ğŸ‘', 'ğŸ‘', '\\U0001faf0', 'ğŸ¤Ÿ', 'ğŸ¤œ',\n",
    "                'ğŸ¤²', 'ğŸ‘‹', 'ğŸ‘ˆ', 'ğŸ¤š', 'ğŸ™Œ', 'ğŸ™', 'ğŸ¤Œ', 'ğŸ¤', '\\U0001faf2', 'ğŸ‘†', 'ğŸ–', 'ğŸ’ª', 'âœŒ', 'ğŸ¤™', 'âœ‹',\n",
    "                'ğŸ‘Š', '\\U0001faf4', 'ğŸ––', 'ğŸ‘', 'â˜', '\\U0001faf3', 'ğŸ‘', 'ğŸ¤›', 'ğŸ‘‡']\n",
    "\n",
    "df['hand_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in hand_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprising_emojis = ['ğŸ¤¨', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜²', 'ğŸ˜§', 'ğŸ˜¦', 'ğŸ˜¨', 'ğŸ˜±', 'ğŸ¤¯', 'ğŸ˜µ', 'ğŸ˜µâ€ğŸ’«', 'ğŸ§']\n",
    "\n",
    "df['surprising_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in surprising_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_emojis = ['ğŸ˜‘', 'ğŸ˜', 'ğŸ˜¤', 'ğŸ˜®â€ğŸ’¨', 'ğŸ¤¬', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ¤¢', 'ğŸ¤®', 'ğŸ‘¿', 'ğŸ˜ˆ']\n",
    "\n",
    "df['angry_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in angry_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibited_emojis = ['ğŸš«', 'ğŸ”‡', 'ğŸ”•', 'ğŸ›‘', 'ğŸ†˜', 'â›”', 'ğŸ›‘', 'ğŸ“›', 'âŒ', 'â­•', 'ğŸ”', 'â˜¢ï¸']\n",
    "\n",
    "df['prohibited_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in prohibited_emojis)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_substrings(row, substrings):\n",
    "    count = 0\n",
    "    for substring in substrings:\n",
    "        count += row.count(substring)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ÙŠØ§Ø±Ø¨'] = df['content'].apply(lambda x: count_substrings(x, ['ÙŠØ§Ø±Ø¨', 'ÙŠØ§ Ø±Ø¨']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ø§Ù„Ø­Ù…Ø¯'] = df['content'].str.count('Ø§Ù„Ø­Ù…Ø¯')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ù„Ù„Ù‡'] = df['content'].str.count('Ù„Ù„Ù‡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = ['Ø¬Ù…ÙŠÙ„', 'Ø¬Ù…Ø§Ù„', 'Ø­Ø¨', 'Ø®ÙŠØ±', 'ØµØ¨Ø§Ø­']\n",
    "df['good_words'] = df['content'].apply(lambda x: count_substrings(x, good_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_words = ['Ø¹ÙŠØ¨', 'ØºÙ„Ø·', 'ØªØ¹Ø¨', 'ÙƒØ¦ÙŠØ¨', 'Ù‚Ø±Ù', 'Ù…Ø±Ø¶', 'Ù…ÙˆØª', 'Ø³ÙŠØ¡', 'Ù…Ø´ÙƒÙ„', 'Ø®Ø±Ø§', 'Ø²ÙØª', 'Ø¸Ù„Ù…', 'ÙƒØ°Ø¨']\n",
    "df['bad_words'] = df['content'].apply(lambda x: count_substrings(x, bad_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## 2. Define Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words(\"arabic\"))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic(text):\n",
    "    return re.sub(r'[^\\u0621-\\u064A\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_redundant_characters(text, number_of_consecutive_characters):\n",
    "    text += \"\\0\"\n",
    "    result = \"\"\n",
    "\n",
    "    count = 1\n",
    "    prev_char = text[0]\n",
    "\n",
    "    for i in range(1, len(text)):\n",
    "        current_char = text[i]\n",
    "        if current_char == prev_char:\n",
    "            count += 1\n",
    "        else:\n",
    "            if count > number_of_consecutive_characters:\n",
    "                result += prev_char\n",
    "            else:\n",
    "                result += prev_char * count\n",
    "\n",
    "            count = 1\n",
    "            prev_char = current_char\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    stemmer = ARLSTem()\n",
    "    words = word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_consecutive_redundant_characters(text, 3)\n",
    "    text = stem_words(text)\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## 3. After Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalized_content\"] = df[\"content\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_length_after\"] = df[\"normalized_content\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_count_after\"] = df[\"normalized_content\"].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Building ML Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['sentiment', 'content', 'normalized_content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(y_test, y_pred):\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"F-measure:\", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**1. Decision Tree (Random Forest) Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8856659059118015\n",
      "Precision: 0.8857288463426933\n",
      "Recall: 0.8856659059118015\n",
      "F-measure: 0.8856338009882809\n",
      "Confusion Matrix:\n",
      " [[5790  834]\n",
      " [ 719 6240]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=123)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**2. XGBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8635058529043658\n",
      "Precision: 0.8635123373424456\n",
      "Recall: 0.8635058529043658\n",
      "F-measure: 0.8634834413694568\n",
      "Confusion Matrix:\n",
      " [[5660  964]\n",
      " [ 890 6069]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_xgb = le.fit_transform(y_train)\n",
    "y_test_xgb = le.transform(y_test)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf = clf.fit(X_train, y_train_xgb)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test_xgb, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**3. Neural Networks Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8614444526246043\n",
      "Precision: 0.8614892148026398\n",
      "Recall: 0.8614444526246043\n",
      "F-measure: 0.8614551589051935\n",
      "Confusion Matrix:\n",
      " [[5708  916]\n",
      " [ 966 5993]]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**4. KNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7963631009349923\n",
      "Precision: 0.7969107407291351\n",
      "Recall: 0.7963631009349923\n",
      "F-measure: 0.7963882673390669\n",
      "Confusion Matrix:\n",
      " [[5361 1263]\n",
      " [1503 5456]]\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**5. Naive Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.783626592063609\n",
      "Precision: 0.8278724586210388\n",
      "Recall: 0.783626592063609\n",
      "F-measure: 0.7747507496508554\n",
      "Confusion Matrix:\n",
      " [[3891 2733]\n",
      " [ 206 6753]]\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
