{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Read Data from the Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets__file = \"Data/PositiveTweets.tsv\"\n",
    "negative_tweets__file = \"Data/NegativeTweets.tsv\"\n",
    "bad_words__file = \"Data/bad_words.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.read_csv(negative_tweets__file, sep='\\t', header=None, names=[\"sentiment\", \"content\"], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv(positive_tweets__file, sep='\\t', header=None, names=[\"sentiment\", \"content\"], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([neg_df, pos_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>#الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>الدودو جايه تكمل علي 💔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22756</th>\n",
       "      <td>pos</td>\n",
       "      <td>السحب الليلة على الايفون .. رتويت للمرفقة وطبق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22757</th>\n",
       "      <td>pos</td>\n",
       "      <td>😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22758</th>\n",
       "      <td>pos</td>\n",
       "      <td>كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>pos</td>\n",
       "      <td>- ألطف صورة ممكن تعبر عن رمضان 💙</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22760</th>\n",
       "      <td>pos</td>\n",
       "      <td>🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                            content\n",
       "0           neg  اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...\n",
       "1           neg  توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...\n",
       "2           neg  #الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...\n",
       "3           neg  نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...\n",
       "4           neg                             الدودو جايه تكمل علي 💔\n",
       "...         ...                                                ...\n",
       "22756       pos  السحب الليلة على الايفون .. رتويت للمرفقة وطبق...\n",
       "22757       pos         😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂\n",
       "22758       pos  كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...\n",
       "22759       pos                   - ألطف صورة ممكن تعبر عن رمضان 💙\n",
       "22760       pos  🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = set()\n",
    "with open(bad_words__file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        bad_words.add(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Adding Features**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_length_before\"] = df[\"content\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_count_before\"] = df[\"content\"].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences_count_before\"] = df[\"content\"].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hashtags_count\"] = df[\"content\"].apply(lambda x: len(re.findall(r'#', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bad_words_count\"] = df[\"content\"].apply(lambda x: len([word for word in word_tokenize(x) if word in bad_words]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emojis_count\"] = df[\"content\"].apply(lambda x: emoji.emoji_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_emojis = ['❣', '💍', '🤎', '💌', '🧡', '💙', '💛', '🤍', '💗', '💓', '💋', '💝', '💚', '🖤', '💟', '🔥',\n",
    "                '\\u200d', '🩹', '💘', '💜', '❤', '💟', '💞', '💕', '💖', '😍', '😘', '😗', '😙', '♥️', '😚', '😽',\n",
    "                '😇', '🫶', '😊', '☺️', '🤗', '🤩', '🥰', '😻', '🙈', '🙊', '🫦', '👄', '🫀', '💏', '💑', '💋']\n",
    "\n",
    "df['love_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in love_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_heart_emoji = ['💔']\n",
    "\n",
    "df['broken_heart'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in broken_heart_emoji)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_emojis = ['😄', '😹', '😃', '😆', '😅', '😀', '🤣', '😂', '😁', '😝', '😸', '😇', '🤪', '😎', '🕺', '💃']\n",
    "\n",
    "df['happy_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in happy_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_emojis = ['\\U0001fae4', '🥲', '😟', '😩', '😢', '😓', '😥', '🙃', '💅', '😞', '😔', '🙁', '☹️', '😿', '😖',\n",
    "                '️💔', '😰', '😭', '😣', '☹', '😕', '🥵', '🥴', '🤕', '🤒']\n",
    "\n",
    "df['sad_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in sad_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_emojis = ['🙂', '🙃']\n",
    "\n",
    "df['smile_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in smile_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinking_emojis = ['🤔', '🤨', '🧐', '🤓']\n",
    "\n",
    "df['thinking_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in thinking_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_emojis = ['💐', '🌸', '🏵️', '🌹', '🌺', '🌻', '🌼', '🌷', '🥀', '☘️', '🍁']\n",
    "\n",
    "df['flowers_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in flowers_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_and_sun_emojis = ['🌝', '🌑', '🌒', '🌓', '🌔', '🌕', '🌖', '🌗', '🌘', '🌙', '🌚', '🌛', '🌜', '☀️', '🌞',\n",
    "                        '⭐', '🌟', '🌠', '✨']\n",
    "\n",
    "df['moon_and_sun_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in moon_and_sun_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_emojis = ['👉', '✊', '👌', '\\U0001faf5', '🤘', '🤞', '🤝', '\\U0001faf1', '👐', '👎', '\\U0001faf0', '🤟', '🤜',\n",
    "                '🤲', '👋', '👈', '🤚', '🙌', '🙏', '🤌', '🤏', '\\U0001faf2', '👆', '🖐', '💪', '✌', '🤙', '✋',\n",
    "                '👊', '\\U0001faf4', '🖖', '👍', '☝', '\\U0001faf3', '👏', '🤛', '👇']\n",
    "\n",
    "df['hand_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in hand_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprising_emojis = ['🤨', '😐', '😑', '😶', '😮', '😯', '😲', '😧', '😦', '😨', '😱', '🤯', '😵', '😵‍💫', '🧐']\n",
    "\n",
    "df['surprising_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in surprising_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_emojis = ['😑', '😐', '😤', '😮‍💨', '🤬', '😡', '😠', '🤢', '🤮', '👿', '😈']\n",
    "\n",
    "df['angry_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in angry_emojis)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibited_emojis = ['🚫', '🔇', '🔕', '🛑', '🆘', '⛔', '🛑', '📛', '❌', '⭕', '🔞', '☢️']\n",
    "\n",
    "df['prohibited_emojis'] = df['content'].apply(\n",
    "    lambda x: sum(emoji in x for emoji in prohibited_emojis)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_substrings(row, substrings):\n",
    "    count = 0\n",
    "    for substring in substrings:\n",
    "        count += row.count(substring)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['يارب'] = df['content'].apply(lambda x: count_substrings(x, ['يارب', 'يا رب']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['الحمد'] = df['content'].str.count('الحمد')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['لله'] = df['content'].str.count('لله')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = ['جميل', 'جمال', 'حب', 'خير', 'صباح']\n",
    "df['good_words'] = df['content'].apply(lambda x: count_substrings(x, good_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_words = ['عيب', 'غلط', 'تعب', 'كئيب', 'قرف', 'مرض', 'موت', 'سيء', 'مشكل', 'خرا', 'زفت', 'ظلم', 'كذب']\n",
    "df['bad_words'] = df['content'].apply(lambda x: count_substrings(x, bad_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## 2. Define Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words(\"arabic\"))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic(text):\n",
    "    return re.sub(r'[^\\u0621-\\u064A\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_redundant_characters(text, number_of_consecutive_characters):\n",
    "    text += \"\\0\"\n",
    "    result = \"\"\n",
    "\n",
    "    count = 1\n",
    "    prev_char = text[0]\n",
    "\n",
    "    for i in range(1, len(text)):\n",
    "        current_char = text[i]\n",
    "        if current_char == prev_char:\n",
    "            count += 1\n",
    "        else:\n",
    "            if count > number_of_consecutive_characters:\n",
    "                result += prev_char\n",
    "            else:\n",
    "                result += prev_char * count\n",
    "\n",
    "            count = 1\n",
    "            prev_char = current_char\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    stemmer = ARLSTem()\n",
    "    words = word_tokenize(text)\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_non_arabic(text)\n",
    "    text = remove_consecutive_redundant_characters(text, 3)\n",
    "    text = stem_words(text)\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## 3. After Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normalized_content\"] = df[\"content\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_length_after\"] = df[\"normalized_content\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_count_after\"] = df[\"normalized_content\"].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "\n",
    "# **Building ML Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['sentiment', 'content', 'normalized_content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(y_test, y_pred):\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"F-measure:\", metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**1. Decision Tree (Random Forest) Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8856659059118015\n",
      "Precision: 0.8857288463426933\n",
      "Recall: 0.8856659059118015\n",
      "F-measure: 0.8856338009882809\n",
      "Confusion Matrix:\n",
      " [[5790  834]\n",
      " [ 719 6240]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=123)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**2. XGBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8635058529043658\n",
      "Precision: 0.8635123373424456\n",
      "Recall: 0.8635058529043658\n",
      "F-measure: 0.8634834413694568\n",
      "Confusion Matrix:\n",
      " [[5660  964]\n",
      " [ 890 6069]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_xgb = le.fit_transform(y_train)\n",
    "y_test_xgb = le.transform(y_test)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf = clf.fit(X_train, y_train_xgb)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test_xgb, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**3. Neural Networks Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8614444526246043\n",
      "Precision: 0.8614892148026398\n",
      "Recall: 0.8614444526246043\n",
      "F-measure: 0.8614551589051935\n",
      "Confusion Matrix:\n",
      " [[5708  916]\n",
      " [ 966 5993]]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**4. KNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7963631009349923\n",
      "Precision: 0.7969107407291351\n",
      "Recall: 0.7963631009349923\n",
      "F-measure: 0.7963882673390669\n",
      "Confusion Matrix:\n",
      " [[5361 1263]\n",
      " [1503 5456]]\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**5. Naive Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.783626592063609\n",
      "Precision: 0.8278724586210388\n",
      "Recall: 0.783626592063609\n",
      "F-measure: 0.7747507496508554\n",
      "Confusion Matrix:\n",
      " [[3891 2733]\n",
      " [ 206 6753]]\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print_measures(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
